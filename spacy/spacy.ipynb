{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ed344f-d855-4a00-bbcf-50d8109213b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d17fd8aa-294a-4206-8c15-da6759cc084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c026b73b-5895-4972-9c51-6bb94b67b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb58ed90-d129-4551-9283-cd0a729d4488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        next_token = doc[token.i + 1]\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a34d4b4-ed57-4e0a-aed9-ec4947608247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852ba920-c50e-465d-ad0a-545c0b5717b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"She ate the pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319a8e3a-9e15-4843-81df-8a4686b21770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb68ad7-998b-4567-93fc-489bc8093a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b33de9e0-b68b-4039-a3c9-396727e68883",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ce0a60-5ffe-42c6-919b-b2fea726e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a5047d-2cbb-438a-8f34-4a499529a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd14cee5-f9a7-4c8e-87a2-a76899a9aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "082f0bad-5d89-473d-9e54-1cdd303360e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1093f052-92aa-49e7-98a8-edb13c38add5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup: France won!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4577384-4113-4ca3-b518-1efa47df7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Importar a classe Doc \n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# As palavras e espaços em branco necessários para criar um doc:\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Criar um doc manualmente\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f61299df-bcac-4e97-9a7e-3081e6d229c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Hello world,)\n"
     ]
    }
   ],
   "source": [
    "# Criar uma particção span manualmente\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Criar uma partição span com um marcador\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Adicionar a partição a doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a7aa46d-f3f6-4ff5-992e-1da3c187c54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu adoro David Bowie\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"pt\")\n",
    "\n",
    "# Importe as classes Doc e Span \n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"Eu\", \"adoro\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Crie um doc Doc a partir das palavras words e espaçamento spaces\n",
    "doc = Doc(nlp.vocab, words = words, spaces = spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Crie uma partição para \"David Bowie\" a partir do doc e atribua o marcador \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "\n",
    "# Adicione a partição às entidades do doc.\n",
    "doc.ents = [span]\n",
    "\n",
    "# Imprima o texto e os marcadores das entidades\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4dab860-75e5-49ae-ab65-9eee981b6311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "for token in doc:\n",
    "    # Verifica se o token atual é um substantivo próprio.\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Verifica se o próximo token é um verbo\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea69f06c-d572-4ee3-8a23-f67039888116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "0.8698332283318978\n"
     ]
    }
   ],
   "source": [
    "# Carregar o fluxo (pipeline) de processamento maior com os vetores\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Comparar dois documentos\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61f6d116-c322-483c-b585-9ad70fa8345b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.685019850730896\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "053fa656-6f81-4cf9-be00-538a9808ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1821369691957915\n"
     ]
    }
   ],
   "source": [
    "# Comparar um documento com um token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf349494-3347-41e5-8c60-e0c4b94760f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47190033157126826\n"
     ]
    }
   ],
   "source": [
    "# Comparar uma partição com um documento\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa6f1715-3016-4ae7-8699-a223ad2a8fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20778  -2.4151    0.36605   2.0139   -0.23752  -3.1952   -0.2952\n",
      "  1.2272   -3.4129   -0.54969   0.32634  -1.0813    0.55626   1.5195\n",
      "  0.97797  -3.1816   -0.37207  -0.86093   2.1509   -4.0845    0.035405\n",
      "  3.5702   -0.79413  -1.7025   -1.6371   -3.198    -1.9387    0.91166\n",
      "  0.85409   1.8039   -1.103    -2.5274    1.6365   -0.82082   1.0278\n",
      " -1.705     1.5511   -0.95633  -1.4702   -1.865    -0.19324  -0.49123\n",
      "  2.2361    2.2119    3.6654    1.7943   -0.20601   1.5483   -1.3964\n",
      " -0.50819   2.1288   -2.332     1.3539   -2.1917    1.8923    0.28472\n",
      "  0.54285   1.2309    0.26027   1.9542    1.1739   -0.40348   3.2028\n",
      "  0.75381  -2.7179   -1.3587   -1.1965   -2.0923    2.2855   -0.3058\n",
      " -0.63174   0.70083   0.16899   1.2325    0.97006  -0.23356  -2.094\n",
      " -1.737     3.6075   -1.511    -0.9135    0.53878   0.49268   0.44751\n",
      "  0.6315    1.4963    4.1725    2.1961   -1.2409    0.4214    2.9678\n",
      "  1.841     3.0133   -4.4652    0.96521  -0.29787   4.3386   -1.2527\n",
      " -1.7734   -3.5637   -0.20035  -3.3013    0.99951  -0.92888  -0.94594\n",
      "  1.5124   -3.9385    2.7935   -3.1042    3.3382    0.54513  -0.37663\n",
      "  2.5151    0.51468  -0.88907   1.011     3.4705   -3.6037    1.3702\n",
      "  2.3468    1.6674    1.3904   -2.8112    2.237    -1.0344   -0.57164\n",
      "  1.0641   -1.6919    1.958    -0.78305   0.14741   0.51083   1.8278\n",
      " -0.69638   0.90548   0.62282  -1.8315   -2.8587    0.48424  -2.0527\n",
      " -0.53808  -2.3472    1.0354   -1.8257   -0.3892   -0.24943   0.8651\n",
      " -1.5195    1.2166   -2.698    -0.96698   2.2175   -0.16089  -0.49677\n",
      " -0.19646   1.3284    4.0824    1.3919    0.80669  -1.0316   -0.28056\n",
      " -1.8632    0.47716  -0.53628   1.3853   -2.1755   -0.2354    2.4933\n",
      " -0.87255   1.4493   -0.10778  -0.44159   1.3462    4.4211   -1.8385\n",
      "  0.3985    0.47637  -0.60074   3.3583   -0.15006  -0.40495   2.7225\n",
      " -1.6297    0.86797  -4.1445   -2.7793    1.1535   -0.011691  0.9792\n",
      " -1.0141    0.80134   0.43642   1.4337    2.8927    0.82871  -1.1827\n",
      " -1.3838    2.3903   -0.89323   1.1461   -1.7435    0.8654   -0.27075\n",
      " -0.78698   1.5631   -0.5923    0.098082 -0.26682   1.6282   -0.77495\n",
      "  3.2552    1.7964   -1.4314    1.2336    2.3102   -1.6328    2.8366\n",
      " -0.71384   0.43967   1.5627    3.079    -0.922    -0.43981  -0.7659\n",
      "  1.9362   -2.2479    1.041     0.63206   1.5855    3.4097   -2.9204\n",
      " -1.4751   -0.59534  -1.688    -4.1362    2.745    -2.8515    3.6509\n",
      " -0.66993  -2.8794    2.0733    1.1779   -2.0307    2.595    -0.12246\n",
      "  1.5844    1.1855    0.022385 -2.2916   -2.2684   -2.7537    0.34981\n",
      " -4.6243   -0.96521  -1.1435   -2.8894   -0.12619   2.9577   -1.7227\n",
      "  0.24757   1.2149    3.5349   -0.95802   0.080346 -1.6553   -0.6734\n",
      "  2.2918   -1.8229   -1.1336    1.8884    2.4789   -0.66061   2.0529\n",
      " -0.76687   0.32362  -2.2579    0.91278   0.36231   0.61562  -0.15396\n",
      " -0.42917  -0.89848   0.17298  -0.76978  -2.0222   -1.7127   -1.5632\n",
      "  0.56631  -1.354     2.6261    1.9156   -1.5651    1.8315   -1.4257\n",
      " -1.6861   -0.51953   1.7635   -0.50722   1.388    -1.1012  ]\n"
     ]
    }
   ],
   "source": [
    "# Carregar um fluxo (pipeline) de processamento maior com vetores\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "# Acessar o vetor através do atributo token.vector\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0541fc7b-a4a2-473b-88cc-03301057d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: \"I love cats and I'm very very happy\"\n",
      "Matches encontrados: \n",
      "\tlove cats\n",
      "\tvery happy\n",
      "\tvery very happy\n"
     ]
    }
   ],
   "source": [
    "# Inicializar com o vocabulario compartilhado\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Expressões são listas de dicionários descrevendo os tokens\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"LOWER\": \"cats\"}]\n",
    "matcher.add(\"LOVE_CATS\",[pattern])\n",
    "\n",
    "# Operadores podem determinar a frequência de correspondência de um token\n",
    "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
    "matcher.add(\"VERY_HAPPY\", [pattern])\n",
    "\n",
    "# Chamar o comparador no documento doc retorna uma lista com tuplas (match_id, start, end) \n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(f'Texto original: \"{doc}\"')\n",
    "print('Matches encontrados: ')\n",
    "for index, start, end in matches:\n",
    "    print(f'\\t{doc[start:end]}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22a84d7b-6d3b-4f50-88a1-b9296ff5441f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Obter o token raiz e o token cabeçalho da partição \n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Obter o token anterior e seu marcador de classe gramatical\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6861e9d-b883-4768-a7b1-637f825c5d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", [pattern])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterar nas correspondências\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Obter a partição que houve correspondência\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f01841-c0db-4887-a2ea-dc032c748c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
