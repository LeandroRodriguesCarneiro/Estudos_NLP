{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6025c10e-1860-4099-9f3d-7a893341f4f3",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural (Natural Language Processing)\n",
    "O computados tem dificuldade em processar dados não estruturados, printipalmento no que se refere a texto e linguagem, com o surjimento da demanda e foram criando formas de fazer o computador processar textos e com o surgimento do machine learning e as redes neurais artificiais trouxe mais possibilidades\n",
    "## Corpus\n",
    "Conjunto de documentos(Textos não estruturado) em linguagem natural\n",
    "\n",
    "## Annotations\n",
    "Localizar e classificar elementos especificos no texto.\n",
    " * Exemplos:\n",
    "    * Anotar sentimentos para treinamento de IA\n",
    "* Podendo ser especifica por dominio ex. Medicina;\n",
    "* Existem empresas especificas especializadas em anotar\n",
    "* Existem ferramentas expecializadas como: Doccano, Brat etc.\n",
    "* Alguns tipos podem ser feitas por maquinas\n",
    "\n",
    "## Tokenização\n",
    "Processo de subdivisão das sentenças em partes: palavras, pontos e símbulos etc.\n",
    "\n",
    "## Parts-of-speech (POS) tagging\n",
    "Adiciona tags a cada tokem por exemplo sé é verbo, substantivo, adjetivo etc.\n",
    "\n",
    "## Lemmatizing (lemma)\n",
    "Trazer a palavra na sua flexão, de modo que possam ser analizadas juntas.\n",
    "\n",
    "## stemming \n",
    "* Corta palavras, buscando ter uma representação raiz unica\n",
    "* Diferentes tecnicas\n",
    "* Lemmatization é mais sofisticado\n",
    "* Amigo, amigos, amiga, amigas => amig\n",
    "\n",
    "## Dependency parsing\n",
    "Busca encontrar relação entre palvras\n",
    "\n",
    "### Ngram\n",
    "* N palavras sonsecutivas\n",
    "* Bigrams e trigrams\n",
    "* 4 ou mais não é utilizado devido a esparsidade\n",
    "* pode ser aplicado em letras\n",
    "\n",
    "## Modelo\n",
    "Um modelo de banco de dados linguistico \n",
    "* Analize:\n",
    "   * Verbo, substantivo, quais são as flexôes? Quais são as dependencias\n",
    "* Especifico para cada idioma\n",
    "* Maioria das bibliotecas de NLP tem seus próprios modelos (ou usam modelos de terceiros)\n",
    "* Pode criar seu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e24b1-f238-4ff6-a12f-74e731429579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "import string\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "spacy.cli.download('pt_core_news_lg')\n",
    "\n",
    "nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c33b1-5a70-475b-b5bb-542fd2d42580",
   "metadata": {},
   "source": [
    "Carregando arquivo de textto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fe649-c584-45f1-8b11-6bef8f4b7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "PATH = os.path.join(base_path,'NLTK_word2vec')\n",
    "\n",
    "with open(os.path.join(PATH,'teste.txt'), 'r') as txt:\n",
    "    text = txt.readlines()\n",
    "    txt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050b40c-49c3-4da8-b928-9c35460953d8",
   "metadata": {},
   "source": [
    "Fazendo a tokenização para o nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253f5d4-e396-400d-9de2-f7a67e2d0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenses = []\n",
    "tokens =[]\n",
    "for sentense in text:\n",
    "    sentense = sentense.replace('\\n', '')\n",
    "    if sentense == ' ':\n",
    "        continue\n",
    "    token_sentense = nltk.sent_tokenize(sentense.lower())\n",
    "    sentenses.append(token_sentense)\n",
    "    for sentense in token_sentense:\n",
    "        tokens.append(nltk.word_tokenize(sentense.lower()))\n",
    "\n",
    "print(sentenses[0][0])\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d25752-c6e3-485c-983f-53c3dcce62c3",
   "metadata": {},
   "source": [
    "Removendo stopwords e pontuações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5556959-c5ed-413c-9c85-cb2900add52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_clear = []\n",
    "for sentense_tokens in tokens:\n",
    "    token_sentense = []\n",
    "    for token in sentense_tokens:\n",
    "        if token not in string.punctuation and token not in stopwords.words('portuguese'):\n",
    "            token_sentense.append(token)\n",
    "    tokens_clear.append(token_sentense)\n",
    "print(tokens_clear[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b35fdc3-537c-4137-8c90-994acf5ee501",
   "metadata": {},
   "source": [
    "Aqui é possivel ver a frquencia das palvras em uma lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c957a30-5a4c-4bd7-9415-8942436a1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [token for sentense_token in tokens_clear for token in sentense_token]\n",
    "frequency = nltk.FreqDist(tokens_list)\n",
    "frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bbe07-8506-4cce-ac80-e5300563756a",
   "metadata": {},
   "source": [
    "O método hapaxes: encontra palavras que aparecem apenas uma vez em um corpus, que podem ser erros de digitação ou termos muito específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710c609-77ce-4f86-a6b9-d276d4a4d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "hapaxes = frequency.hapaxes()\n",
    "print(hapaxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9af78-b1fb-4ee8-8374-1f33ec00570b",
   "metadata": {},
   "source": [
    "O FreqDist também pode ser utilizado para comparar as distribuições de frequência de diferentes textos para identificar diferenças estilísticas ou temáticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dedd980-3bc8-495c-9c11-0784a21397e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sentense1 = nltk.FreqDist(tokens_clear[0])\n",
    "for i, sentense in enumerate(tokens_clear):\n",
    "    if not i == 0:\n",
    "        freq_sentense2 = nltk.FreqDist(tokens_clear[i])\n",
    "        common_words = set(freq_sentense1).intersection(set(freq_sentense2))\n",
    "        print(tokens_clear[i], ' - ',common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8635f-9739-4bc4-8118-d5274ecf6356",
   "metadata": {},
   "source": [
    "## utilizando o PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d20a22-e1ea-429c-9ca1-2e531dfe3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stem1 = [stemmer.stem(token) for token in tokens_list]\n",
    "print(stem1[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff75823-707e-4ab7-a3c1-f0dbb82ff51e",
   "metadata": {},
   "source": [
    "## Utilizando o SnowBallStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49cf06-647b-49fb-a92d-47425403d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer('portuguese')\n",
    "stem2 = [stemmer2.stem(token) for token in tokens_list]\n",
    "print(stem2[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3bd53-1615-4f8f-9f79-d19a7e6e5e74",
   "metadata": {},
   "source": [
    "## utilizando o LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5048474-2623-427c-bd9e-5ba5afa21be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer3 = LancasterStemmer()\n",
    "stem3 = [stemmer3.stem(token) for token in tokens_list]\n",
    "print(stem3[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cdf13-fe40-45d0-829c-ae4aa6c5ee41",
   "metadata": {},
   "source": [
    "## Utilizando o lemmatizer\n",
    "Aprarentemente o lemmatizer não está funcionando corretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696ed34-6a69-47e8-a5f9-de7c5aa9bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatize = [lemmatizer.lemmatize(token) for token in tokens_list]\n",
    "print(lemmatize[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f3ec2-b819-4868-b917-1f9cac7962da",
   "metadata": {},
   "source": [
    "O postagging do NLTK não está disponivel em portugues apenas em inglês e russo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fc70d-c0a8-40d3-8b18-a5a5e57aae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = nltk.pos_tag(tokens, lang='port')\n",
    "# print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e4a8c-9628-4181-a106-742674535957",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = ['palavra', 'PorterStemmer', 'SnowBallStemmer', 'LancasterStemmer', 'lemmatizer_NLTK']\n",
    "data = []\n",
    "for i, token in enumerate(tokens_list):\n",
    "    data.append([token, stem1[i], stem2[i], stem3[i], lemmatize[i]])\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns_names)\n",
    "display(df.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368b64d-a7e9-4f46-ade0-86ad510fab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenses = [sentense.replace('\\n', '').lower() for sentense in text  if not sentense == ' \\n']\n",
    "tokens_spacy = []\n",
    "for sentense in sentenses:\n",
    "    doc = nlp(sentense)\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            tokens_spacy.append(token)\n",
    "print(tokens_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513839c-a8ce-479b-83e0-b75124a39159",
   "metadata": {},
   "outputs": [],
   "source": [
    "commun = [token_spacy for token_spacy in tokens_spacy if str(token_spacy) in tokens_list]\n",
    "tokens_strings  = set([token.text for token in commun])\n",
    "df_words = set([word for word in df['palavra']])\n",
    "\n",
    "remove = df_words.difference(tokens_strings)\n",
    "\n",
    "keep = tokens_strings.difference(remove)\n",
    "print(len(df['palavra']) - len(commun))\n",
    "\n",
    "mask = df['palavra'].isin(list(keep))\n",
    "df = df[mask]\n",
    "\n",
    "df['lemmatizer_spacy'] = [token.lemma_  for token in commun]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ff663-e0be-4821-b705-07bcd318d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
