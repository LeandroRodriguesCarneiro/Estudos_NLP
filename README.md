# üìö Estudos em Processamento de Linguagem Natural (NLP)

Este reposit√≥rio re√∫ne estudos pr√°ticos em Processamento de Linguagem Natural (NLP), explorando bibliotecas populares como **NLTK**, **spaCy** e **Word2Vec**. O objetivo √© consolidar conhecimentos te√≥ricos por meio de implementa√ß√µes pr√°ticas em Python. 

### Algumas defini√ß√µes
- Processamento de Linguagem Natural (Natural Language Processing): O computados tem dificuldade em processar dados n√£o estruturados, printipalmento no que se refere a texto e linguagem, com o surjimento da demanda e foram criando formas de fazer o computador processar textos e com o surgimento do machine learning e as redes neurais artificiais trouxe mais possibilidades
- Corpus: Conjunto de documentos(Textos n√£o estruturado) em linguagem natural

- Annotations: Localizar e classificar elementos especificos no texto.
   * Exemplos:
      * Anotar sentimentos para treinamento de IA
  * Podendo ser especifica por dominio ex. Medicina;
  * Existem empresas especificas especializadas em anotar
  * Existem ferramentas expecializadas como: Doccano, Brat etc.
  * Alguns tipos podem ser feitas por maquinas

- Tokeniza√ß√£o: Processo de subdivis√£o das senten√ßas em partes: palavras, pontos e s√≠mbulos etc.

- Parts-of-speech (POS) tagging: Adiciona tags a cada tokem por exemplo s√© √© verbo, substantivo, adjetivo etc.

- Lemmatizing (lemma): Trazer a palavra na sua flex√£o, de modo que possam ser analizadas juntas.

- stemming:
  * Corta palavras, buscando ter uma representa√ß√£o raiz unica
  * Diferentes tecnicas
  * Lemmatization √© mais sofisticado
  * Amigo, amigos, amiga, amigas => amig

- Dependency parsing: Busca encontrar rela√ß√£o entre palvras

- Ngram:
  * N palavras sonsecutivas
  * Bigrams e trigrams
  * 4 ou mais n√£o √© utilizado devido a esparsidade
  * pode ser aplicado em letras

- Modelo:
  Um modelo de banco de dados linguistico 
  * Analize:
     * Verbo, substantivo, quais s√£o as flex√¥es? Quais s√£o as dependencias
  * Especifico para cada idioma
  * Maioria das bibliotecas de NLP tem seus pr√≥prios modelos (ou usam modelos de terceiros)
  * Pode criar seu

- Word Embedding: √â representa√ß√µes computacionais de texto:
  * independente de contexto
  * dependente de contexto

- One-hot encode: √â uma representa√ß√£o matricial de uma fraze em uma matriz formada por zeros e uns

- TF-IDF: √â feita pela frequencia das palavras utilizadas na frase palavras mais frequentes tendem a se aproximar de 0 e palavras menos frequentes tendem a ter valor mais alto

- word2vec: Atravez de um processo de treinamento, produz um vetor demonstrando matematicamente a rela√ß√£o entre palavras existem duas formas principais:
  * CBOW: busca prever uma palavra baseadono contexto
  * Skip-gram: busca prever o contexto a partir de uma palavra central

### O que encontrar√° nesse projeto:
- nltk/: Explora√ß√µes com a biblioteca NLTK, incluindo tokeniza√ß√£o, stemming, lematiza√ß√£o e an√°lise de sentimentos.
- spacy/: Aplica√ß√µes com spaCy, abordando reconhecimento de entidades nomeadas (NER), depend√™ncias sint√°ticas e vetores de palavras.
- word2vec/: Implementa√ß√µes de modelos Word2Vec para representa√ß√£o vetorial de palavras juntamente com um classificador de NER desenvolvido por mim.
- classificador_span/: Desenvolvimento de um classificador de sentimentos utilizando spaCy e dados em espanhol.
